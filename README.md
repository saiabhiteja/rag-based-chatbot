# RAGâ€‘Chatbot

A lightweight **Retrievalâ€‘Augmented Generation (RAG) chatbot** built with **Streamlit**, **LangChain**, **Pinecone**, and **OpenAI**.  
Ask questions about any collection of PDFs and get grounded answers with cited sources in real time.

[Live demo](https://rag-chatbot-saketh.streamlit.app/) 

* Built with â¤ï¸ by **Saketh Angirekula**

---

## âœ¨ Features
* **Dragâ€‘andâ€‘drop document ingestion** â€“ drop PDFs in `pdf/`, click **Embed Docs** and they are chunked, embedded and pushed to Pinecone.  
* **Hybrid search â†’ generative answers** â€“ combines semantic vector search with an LLM to produce accurate, sourceâ€‘linked responses.  
* **Multiâ€‘page Streamlit UI** â€“ switch between Chat, Document Upload and Settings from the sidebar.  
* **Stateless & serverless friendly** â€“ no database other than Pinecone; ready for Streamlit Community Cloud or container deploy.  
* **Extensible LangChain pipeline** â€“ swap models, reâ€‘rankers or vector stores with just a few lines of code.

---

## ğŸ— Architecture

```mermaid
flowchart LR
    subgraph Client
        A(User) -->|question| B(UI â€“ Streamlit)
    end
    subgraph Backend
        B -->|vector query| C[Pinecone Index]
        C -->|topâ€‘k docs| D(LangChain Retriever)
        D -->|context| E(OpenAI LLM)
        E -->|answer+sources| B
    end
```
## ğŸ“‚ Project structure

| Path               | Purpose                                                               |
| ------------------ | --------------------------------------------------------------------- |
| `home.py`          | Landing page & global Streamlit config                                |
| `pages/`           | Additional Streamlit pages (`Chat.py`, `Upload.py`, `Settings.py`, â€¦) |
| `functions/`       | Utility modules â€” `chunks.py`, `embed.py`, `qa_chain.py`, etc.        |
| `pdf/`             | Sample documents and any files you upload                             |
| `requirements.txt` | Minimal dependency pinning                                            |

## âš¡ Quickâ€‘start

```bash

# 1Â Clone & set up
git clone https://github.com/Saketh1702/RAG-chatbot.git
cd RAG-chatbot
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt     # PythonÂ 3.10+

# 2Â Add your secrets
export OPENAI_API_KEY="<yourâ€‘key>"
export PINECONE_API_KEY="<yourâ€‘key>"
export PINECONE_ENV="gcp-starter"
export PINECONE_INDEX="rag-chatbot"

# 3Â Ingest documents (optional â€“ adds all PDFs in pdf/)
python functions/embed.py

# 4Â Run the app
streamlit run home.py
```
Tip: Create a .env file and useÂ pythonâ€‘dotenv if you donâ€™t want to export keys every time.

## ğŸ”§ Configuration

| Variable           | Description                 |
| ------------------ | --------------------------- |
| `OPENAI_API_KEY`   | Key for chat / embeddings   |
| `PINECONE_API_KEY` | Vector store access         |
| `PINECONE_ENV`     | Region (e.g.Â `gcp-starter`) |
| `PINECONE_INDEX`   | Index name to read/write    |

Model & retriever parameters (chunk size, overlap, topâ€‘k, temperature) live in functions/qa_chain.py.

## ğŸš€ Deployment
Streamlit Community Cloud

1. Fork the repository.

2. Add Secrets (OPENAI_API_KEY, PINECONE_API_KEY, â€¦) in the dashboard.

3. Click Deploy â€“ done!

### Docker
```bash
docker build -t rag-chatbot .
docker run -p 8501:8501 \
  -e OPENAI_API_KEY -e PINECONE_API_KEY -e PINECONE_ENV -e PINECONE_INDEX \
  rag-chatbot
```
## ğŸ¤ Contributing
Pull requests are welcome! Please open an issue first to discuss major changes.

Bug fix â€“ branch from main, write a test, open PR

Feature â€“ describe useâ€‘case in an issue and wait for approval

Docs â€“ typos & clarifications are highly appreciated


## ğŸ™ Acknowledgements
[Streamlit](https://streamlit.io) â€“ rapid data apps

[LangChain](https://python.langchain.com/) â€“ LLM orchestration

[Pinecone](https://www.pinecone.io/) â€“ managed vector database

[OpenAI](https://openai.com/) â€“ GPT & embeddings API

